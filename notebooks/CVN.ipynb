{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_Block(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            output_1x1,\n",
    "            output_1x1_block2,\n",
    "            output_3x3,\n",
    "            output_5x5_reduce,\n",
    "            output_5x5,\n",
    "            output_pool,\n",
    "    ):\n",
    "            super(Inception_Block, self).__init__()\n",
    "            self.branch1 = ConvBlock(in_channels, output_1x1, kernel_size = 1)\n",
    "            self.branch2 = nn.Sequential(\n",
    "            ConvBlock(in_channels, out_channels = output_1x1_block2, kernel_size = 1),\n",
    "            ConvBlock(output_1x1_block2, output_3x3, kernel_size = 3, padding = 1)\n",
    "            )\n",
    "            self.branch3 = nn.Sequential(\n",
    "                  ConvBlock(in_channels, out_channels = output_5x5_reduce, kernel_size = 1),\n",
    "                  ConvBlock(in_channels= output_5x5_reduce, out_channels=output_5x5, kernel_size = 5, padding = 2),\n",
    "            )\n",
    "            self.branch4 = nn.Sequential(\n",
    "                  nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                  ConvBlock(in_channels, output_pool, kernel_size = 1)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "          first_block = self.branch1(x)\n",
    "          second_block = self.branch2(x)\n",
    "          third_block = self.branch3(x)\n",
    "          fourth_block = self.branch4(x)\n",
    "          output_concat = torch.cat([first_block, second_block, third_block, fourth_block], dim  = 1)\n",
    "\n",
    "          return output_concat\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class x_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(x_model,self).__init__()\n",
    "        self.conv_7x7 = nn.Conv2d(in_channels=1, out_channels=6,  kernel_size=7, stride=2)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride = 2)        \n",
    "        self.lrn_norm = nn.LocalResponseNorm(size=5,  alpha=0.0001, beta=0.75)\n",
    "        self.conv_1x1 = nn.Conv2d(in_channels=6, out_channels= 12, kernel_size=1)\n",
    "        self.conv3_3x3 = nn.Conv2d(in_channels=12, out_channels=6, kernel_size=3)\n",
    "        # self.inception3a = Inception_Block(in_channels=6,output_1x1=64, output_1x1_block2=96, output_3x3=128, output_5x5_reduce=16, output_5x5= 32, output_pool=32)\n",
    "        # self.inception3b = Inception_Block(in_channels=256,output_1x1=64, output_1x1_block2=96, output_3x3=128, output_5x5_reduce=16, output_5x5= 32, output_pool=32)\n",
    "        # self.inception4a = Inception_Block(in_channels=256,output_1x1=64, output_1x1_block2=96, output_3x3=128, output_5x5_reduce=16, output_5x5= 32, output_pool=32)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.max_pool(F.relu(self.conv_7x7(x)))\n",
    "        x = self.lrn_norm(x)\n",
    "        x = F.relu(self.conv_1x1(x))\n",
    "        x = F.relu(self.conv3_3x3(x))\n",
    "        x = self.max_pool(self.lrn_norm(x))\n",
    "        # x = self.inception3a(x)\n",
    "        # x = self.inception3b(x)\n",
    "        # x = self.max_pool(x)\n",
    "        # x = self.inception4a(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Dimensions of Inception Network and X_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 29, 29])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hep_data = torch.randn(1,1, 256, 256)\n",
    "model_x = x_model()\n",
    "output = model_x(hep_data)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception 3a Dimensions: torch.Size([1, 256, 29, 29])\n",
      "Inception 3b Dimensions: torch.Size([1, 256, 29, 29])\n",
      "Output after MaxPool: torch.Size([1, 256, 14, 14])\n",
      "Inception 4a Dimensions: torch.Size([1, 256, 14, 14])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inception3a = Inception_Block(in_channels=6,output_1x1=64, output_1x1_block2=96, output_3x3=128, output_5x5_reduce=16, output_5x5= 32, output_pool=32)\n",
    "output3a = inception3a(output)\n",
    "print(f'Inception 3a Dimensions: {output3a.shape}')\n",
    "inception3b = Inception_Block(in_channels=256,output_1x1=64, output_1x1_block2=96, output_3x3=128, output_5x5_reduce=16, output_5x5= 32, output_pool=32)\n",
    "output3b = inception3b(output3a)\n",
    "print(f'Inception 3b Dimensions: {output3b.shape}')\n",
    "pool = nn.MaxPool2d(stride=2, kernel_size=3)\n",
    "max_pool_output = pool(output3b)\n",
    "print(f'Output after MaxPool: {max_pool_output.shape}')\n",
    "inception4a = Inception_Block(in_channels=256,output_1x1=64, output_1x1_block2=96, output_3x3=128, output_5x5_reduce=16, output_5x5= 32, output_pool=32)\n",
    "output4a = inception4a(max_pool_output)\n",
    "print(f'Inception 4a Dimensions: {output4a.shape}')\n",
    "concat = torch.cat([output4a, output4a], dim  = 1)\n",
    "combined_inception = Inception_Block(in_channels=512,output_1x1=128, output_1x1_block2=96, output_3x3=256, output_5x5_reduce=16, output_5x5= 64, output_pool=64)\n",
    "combined_output = combined_inception(concat)\n",
    "avg = nn.AvgPool2d(kernel_size=(6,5))\n",
    "final_output = avg(combined_output)\n",
    "reshape_output = final_output.reshape(final_output.shape[0],-1)\n",
    "reshape_output.shape\n",
    "linear_layer = nn.Linear(in_features=2048, out_features=5)\n",
    "final_output = linear_layer(reshape_output)\n",
    "final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class combineXY(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(combineXY, self).__init__()\n",
    "        self.x_model = x_model()\n",
    "        self.y_model = x_model()\n",
    "        # concatenating both models gives us channels of 512\n",
    "        self.final_inception = Inception_Block(in_channels=512,output_1x1=64, output_1x1_block2=96, output_3x3=128, output_5x5_reduce=16, output_5x5= 32, output_pool=32)\n",
    "        self.avg_pooling = nn.AvgPool2d(kernel_size=(6,5))\n",
    "        # self.softmax = nn.Softmax(dim=2)\n",
    "    def forward(self, x_data, y_data):\n",
    "        x = self.x_model(x_data)\n",
    "        y = self.y_model(y_data)\n",
    "        concat =  torch.cat([x, y], dim  = 1)\n",
    "        combined_data = self.final_inception(concat)\n",
    "        combined_data = self.avg_pooling(combined_data)\n",
    "        #combined_data = self.softmax(combined_data)\n",
    "        return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hep_data = torch.randn(1,1, 256, 256)\n",
    "y_hep_data = torch.randn(1,1, 256, 256)\n",
    "\n",
    "combined_model = combineXY()\n",
    "output_combined = combined_model(x_hep_data, y_hep_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 2, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "combineXY                                [1, 256, 2, 2]            --\n",
       "├─x_model: 1-1                           [1, 256, 14, 14]          --\n",
       "│    └─Conv2d: 2-1                       [1, 6, 125, 125]          300\n",
       "│    └─MaxPool2d: 2-2                    [1, 6, 62, 62]            --\n",
       "│    └─LocalResponseNorm: 2-3            [1, 6, 62, 62]            --\n",
       "│    └─Conv2d: 2-4                       [1, 12, 62, 62]           84\n",
       "│    └─Conv2d: 2-5                       [1, 6, 60, 60]            654\n",
       "│    └─LocalResponseNorm: 2-6            [1, 6, 60, 60]            --\n",
       "│    └─MaxPool2d: 2-7                    [1, 6, 29, 29]            --\n",
       "│    └─Inception_Block: 2-8              [1, 256, 29, 29]          --\n",
       "│    │    └─ConvBlock: 3-1               [1, 64, 29, 29]           448\n",
       "│    │    └─Sequential: 3-2              [1, 128, 29, 29]          111,392\n",
       "│    │    └─Sequential: 3-3              [1, 32, 29, 29]           12,944\n",
       "│    │    └─Sequential: 3-4              [1, 32, 29, 29]           224\n",
       "│    └─Inception_Block: 2-9              [1, 256, 29, 29]          --\n",
       "│    │    └─ConvBlock: 3-5               [1, 64, 29, 29]           16,448\n",
       "│    │    └─Sequential: 3-6              [1, 128, 29, 29]          135,392\n",
       "│    │    └─Sequential: 3-7              [1, 32, 29, 29]           16,944\n",
       "│    │    └─Sequential: 3-8              [1, 32, 29, 29]           8,224\n",
       "│    └─MaxPool2d: 2-10                   [1, 256, 14, 14]          --\n",
       "│    └─Inception_Block: 2-11             [1, 256, 14, 14]          --\n",
       "│    │    └─ConvBlock: 3-9               [1, 64, 14, 14]           16,448\n",
       "│    │    └─Sequential: 3-10             [1, 128, 14, 14]          135,392\n",
       "│    │    └─Sequential: 3-11             [1, 32, 14, 14]           16,944\n",
       "│    │    └─Sequential: 3-12             [1, 32, 14, 14]           8,224\n",
       "├─x_model: 1-2                           [1, 256, 14, 14]          --\n",
       "│    └─Conv2d: 2-12                      [1, 6, 125, 125]          300\n",
       "│    └─MaxPool2d: 2-13                   [1, 6, 62, 62]            --\n",
       "│    └─LocalResponseNorm: 2-14           [1, 6, 62, 62]            --\n",
       "│    └─Conv2d: 2-15                      [1, 12, 62, 62]           84\n",
       "│    └─Conv2d: 2-16                      [1, 6, 60, 60]            654\n",
       "│    └─LocalResponseNorm: 2-17           [1, 6, 60, 60]            --\n",
       "│    └─MaxPool2d: 2-18                   [1, 6, 29, 29]            --\n",
       "│    └─Inception_Block: 2-19             [1, 256, 29, 29]          --\n",
       "│    │    └─ConvBlock: 3-13              [1, 64, 29, 29]           448\n",
       "│    │    └─Sequential: 3-14             [1, 128, 29, 29]          111,392\n",
       "│    │    └─Sequential: 3-15             [1, 32, 29, 29]           12,944\n",
       "│    │    └─Sequential: 3-16             [1, 32, 29, 29]           224\n",
       "│    └─Inception_Block: 2-20             [1, 256, 29, 29]          --\n",
       "│    │    └─ConvBlock: 3-17              [1, 64, 29, 29]           16,448\n",
       "│    │    └─Sequential: 3-18             [1, 128, 29, 29]          135,392\n",
       "│    │    └─Sequential: 3-19             [1, 32, 29, 29]           16,944\n",
       "│    │    └─Sequential: 3-20             [1, 32, 29, 29]           8,224\n",
       "│    └─MaxPool2d: 2-21                   [1, 256, 14, 14]          --\n",
       "│    └─Inception_Block: 2-22             [1, 256, 14, 14]          --\n",
       "│    │    └─ConvBlock: 3-21              [1, 64, 14, 14]           16,448\n",
       "│    │    └─Sequential: 3-22             [1, 128, 14, 14]          135,392\n",
       "│    │    └─Sequential: 3-23             [1, 32, 14, 14]           16,944\n",
       "│    │    └─Sequential: 3-24             [1, 32, 14, 14]           8,224\n",
       "├─Inception_Block: 1-3                   [1, 256, 14, 14]          --\n",
       "│    └─ConvBlock: 2-23                   [1, 64, 14, 14]           --\n",
       "│    │    └─Conv2d: 3-25                 [1, 64, 14, 14]           32,832\n",
       "│    └─Sequential: 2-24                  [1, 128, 14, 14]          --\n",
       "│    │    └─ConvBlock: 3-26              [1, 96, 14, 14]           49,248\n",
       "│    │    └─ConvBlock: 3-27              [1, 128, 14, 14]          110,720\n",
       "│    └─Sequential: 2-25                  [1, 32, 14, 14]           --\n",
       "│    │    └─ConvBlock: 3-28              [1, 16, 14, 14]           8,208\n",
       "│    │    └─ConvBlock: 3-29              [1, 32, 14, 14]           12,832\n",
       "│    └─Sequential: 2-26                  [1, 32, 14, 14]           --\n",
       "│    │    └─MaxPool2d: 3-30              [1, 512, 14, 14]          --\n",
       "│    │    └─ConvBlock: 3-31              [1, 32, 14, 14]           16,416\n",
       "├─AvgPool2d: 1-4                         [1, 256, 2, 2]            --\n",
       "==========================================================================================\n",
       "Total params: 1,190,380\n",
       "Trainable params: 1,190,380\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 637.24\n",
       "==========================================================================================\n",
       "Input size (MB): 0.52\n",
       "Forward/backward pass size (MB): 14.22\n",
       "Params size (MB): 4.76\n",
       "Estimated Total Size (MB): 19.50\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(combined_model, input_size=[(1, 1,256,256),(1, 1,256,256)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class_correct = [0 for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_class_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0828, -0.2701,  0.4504, -0.1082],\n",
       "        [ 1.7933, -1.5041, -0.5751, -0.0496],\n",
       "        [ 2.0145, -0.0067,  0.5319, -1.5128],\n",
       "        [ 2.9455, -0.9659,  1.8240,  0.9504],\n",
       "        [-0.1594,  0.3637,  0.4706,  1.5600]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(5, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([0.4504, 1.7933, 2.0145, 2.9455, 1.5600]),\n",
      "indices=tensor([2, 0, 0, 0, 3]))\n"
     ]
    }
   ],
   "source": [
    "m = torch.max(a,1)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb Cell 16\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=176'>177</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=177'>178</a>\u001b[0m \u001b[39m# Forward Pass\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=179'>180</a>\u001b[0m outputs \u001b[39m=\u001b[39m combined_model(images[:, :\u001b[39m2\u001b[39;49m, :, :])\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=180'>181</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m \u001b[39m#Backward and optimize\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb Cell 16\u001b[0m in \u001b[0;36mcombineXY.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m y_data \u001b[39m=\u001b[39m split[\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m \u001b[39m#print(f'x data shape: {x_data.shape}')\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m \u001b[39m#print(f'y data shape: {y_data.shape}')\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_model(x_data)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_model(y_data)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m \u001b[39m#print(f'x data shape: {x.shape}')\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m \u001b[39m#print(f'y data shape: {y.shape}')\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb Cell 16\u001b[0m in \u001b[0;36mx_model.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_pool(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlrn_norm(x))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minception3a(x)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minception3b(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_pool(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minception4a(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb Cell 16\u001b[0m in \u001b[0;36mInception_Block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m second_block \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbranch2(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m third_block \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbranch3(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m fourth_block \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbranch4(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m output_concat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([first_block, second_block, third_block, fourth_block], dim  \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output_concat\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb Cell 16\u001b[0m in \u001b[0;36mConvBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cpare/repos/phys139-239_final_project/notebooks/CVN.ipynb#X22sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/conv.py:446\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/conv.py:442\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    440\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    441\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 442\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    443\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "\n",
    "#Dataset for hdf5 in torch based on lazy loading described here https://vict0rs.ch/2021/06/15/pytorch-h5/\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class hdf5Dataset(Dataset):\n",
    "    def __init__(self, h5_path, x_name, y_name):\n",
    "        super().__init__()\n",
    "        self.h5_path = h5_path\n",
    "        self._data = None\n",
    "        self.x_name = x_name\n",
    "        self.y_name = y_name\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        if self._data is None:\n",
    "            self._data = h5py.File(self.h5_path, \"r\")\n",
    "        return self._data       \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[self.x_name][index], self.data[self.y_name][index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[self.x_name])        \n",
    "    \n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))\n",
    "    \n",
    "class Inception_Block(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            output_1x1,\n",
    "            output_1x1_block2,\n",
    "            output_3x3,\n",
    "            output_5x5_reduce,\n",
    "            output_5x5,\n",
    "            output_pool,\n",
    "    ):\n",
    "            super(Inception_Block, self).__init__()\n",
    "            self.branch1 = ConvBlock(in_channels, output_1x1, kernel_size = 1)\n",
    "            self.branch2 = nn.Sequential(\n",
    "            ConvBlock(in_channels, out_channels = output_1x1_block2, kernel_size = 1),\n",
    "            ConvBlock(output_1x1_block2, output_3x3, kernel_size = 3, padding = 1)\n",
    "            )\n",
    "            self.branch3 = nn.Sequential(\n",
    "                  ConvBlock(in_channels, out_channels = output_5x5_reduce, kernel_size = 1),\n",
    "                  ConvBlock(in_channels= output_5x5_reduce, out_channels=output_5x5, kernel_size = 5, padding = 2),\n",
    "            )\n",
    "            self.branch4 = nn.Sequential(\n",
    "                  nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                  ConvBlock(in_channels, output_pool, kernel_size = 1)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "          first_block = self.branch1(x)\n",
    "          second_block = self.branch2(x)\n",
    "          third_block = self.branch3(x)\n",
    "          fourth_block = self.branch4(x)\n",
    "          output_concat = torch.cat([first_block, second_block, third_block, fourth_block], dim  = 1)\n",
    "\n",
    "          return output_concat\n",
    "    \n",
    "\n",
    "class x_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(x_model,self).__init__()\n",
    "        self.conv_7x7 = nn.Conv2d(in_channels=1, out_channels=64,  kernel_size=7, stride=2)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride = 2)        \n",
    "        self.lrn_norm = nn.LocalResponseNorm(size=5,  alpha=0.0001, beta=0.75)\n",
    "        self.conv_1x1 = nn.Conv2d(in_channels=64, out_channels= 64, kernel_size=1)\n",
    "        self.conv3_3x3 = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3)\n",
    "        self.inception3a = Inception_Block(in_channels=256,output_1x1=64, output_1x1_block2=96, output_3x3=128, output_5x5_reduce=16, output_5x5= 32, output_pool=32)\n",
    "        self.inception3b = Inception_Block(in_channels=256,output_1x1=64, output_1x1_block2=96, output_3x3=128, output_5x5_reduce=16, output_5x5= 32, output_pool=32)\n",
    "        self.inception4a = Inception_Block(in_channels=256,output_1x1=64, output_1x1_block2=96, output_3x3=128, output_5x5_reduce=16, output_5x5= 32, output_pool=32)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.max_pool(F.relu(self.conv_7x7(x)))\n",
    "        x = self.lrn_norm(x)\n",
    "        x = F.relu(self.conv_1x1(x))\n",
    "        x = F.relu(self.conv3_3x3(x))\n",
    "        x = self.max_pool(self.lrn_norm(x))\n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.inception4a(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class combineXY(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(combineXY, self).__init__()\n",
    "        self.x_model = x_model()\n",
    "        self.y_model = x_model()\n",
    "        # concatenating both models gives us channels of 512\n",
    "        self.final_inception = Inception_Block(in_channels=512,output_1x1=128, output_1x1_block2=192, output_3x3=256, output_5x5_reduce=32, output_5x5= 64, output_pool=64)\n",
    "        self.avg_pooling = nn.AvgPool2d(kernel_size=(6,5))\n",
    "        self.linear1 = nn.Linear(2048, 2048)\n",
    "        self.linear2 = nn.Linear(2048, 5)\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, data):\n",
    "        #print(f'data shape: {data.shape}')\n",
    "        split = torch.tensor_split(data, 2, dim = 1)\n",
    "        x_data = split[0]\n",
    "        y_data = split[1]\n",
    "        #print(f'x data shape: {x_data.shape}')\n",
    "        #print(f'y data shape: {y_data.shape}')\n",
    "        x = self.x_model(x_data)\n",
    "        y = self.y_model(y_data)\n",
    "        #print(f'x data shape: {x.shape}')\n",
    "        #print(f'y data shape: {y.shape}')\n",
    "        concat =  torch.cat([x, y], dim  = 1)\n",
    "        #print(f'concat data shape: {concat.shape}')\n",
    "        combined_data = self.final_inception(concat)\n",
    "        combined_data = self.avg_pooling(combined_data)\n",
    "        #print(f'output shape after pooling {combined_data.shape}')\n",
    "        combined_data = combined_data.reshape(combined_data.shape[0],-1)\n",
    "        #print(f'output after reshape {combined_data.shape}')\n",
    "        combined_data = F.relu(self.linear1(combined_data))\n",
    "        combined_data = F.relu(self.linear2(combined_data))\n",
    "\n",
    "        # combined_data = self.softmax(combined_data)\n",
    "        return combined_data\n",
    "N = 320   \n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "dataset_train = hdf5Dataset(\"data/hdf5/train_small.h5\", \"X_train\", \"y_train\")\n",
    "dataset_test = hdf5Dataset(\"data/hdf5/test_small.h5\", \"X_test\", \"y_test\")\n",
    "train_loader = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=True)\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "\n",
    "combined_model = combineXY().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(combined_model.parameters(), lr=learning_rate)\n",
    "\n",
    "best = 100000\n",
    "\n",
    "f = open(\"csv_logs/cvn.csv\", \"w+\")\n",
    "f.write(\"epoch,loss,accuracy,val_loss,val_acc\\n\")\n",
    "f.close()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward Pass\n",
    "        \n",
    "        outputs = combined_model(images[:, :2, :, :])\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #Backward and optimize\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        outputs = torch.softmax(outputs,1)\n",
    "        n_samples = labels.size(0)\n",
    "        correct += (outputs > 0.5).sum().item()\n",
    "        # correct += (outputs == labels).float().sum()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    torch.save(combined_model.state_dict(), f\"models/cvn/cvn_trainsmall_latest.pt\")\n",
    "    if(epoch_loss < best):\n",
    "        best = epoch_loss\n",
    "        torch.save(combined_model.state_dict(), f\"models/cvn/cvn_trainsmall_best.pt\")\n",
    "    epoch_accuracy = 100.0 * correct / n_samples\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = combined_model(images[:, :2, :, :])\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        outputs = torch.softmax(outputs,1)\n",
    "        n_samples = labels.size(0)\n",
    "        correct += (outputs > 0.5).sum().item()\n",
    "    val_loss = running_loss / len(test_loader)\n",
    "    val_accuracy = 100.0 * correct / n_samples\n",
    "\n",
    "    f = open(\"csv_logs/cvn.csv\", \"a\")\n",
    "    f.write(f\"Epoch {epoch}: Loss = {epoch_loss},train acc = {epoch_accuracy},Val Loss = {val_loss}, Val acc = {val_accuracy}\\n\")\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
